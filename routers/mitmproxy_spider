#!/usr/bin/python
import os
import argparse
import requests


def find_web_paths(mitmproxy_urls):
    for line in open(os.path.expanduser(mitmproxy_urls)):
        yield line.rstrip("\n")
               
def request(url):
    http = requests.get(url, allow_redirects=False)
    return http.status_code, http.content

def main(mitmproxy_urls, exclude):
    pages = {}
    # parse all pages
    for url in find_web_paths(mitmproxy_urls):
        code, content = request(url)
        # filter
        if not exclude and content in pages:
            print code, url, "same content as %s" % pages[content]
        elif not exclude and code != 200:
            pages[content] = url
            print code, url
        elif not exclude and code == 200:
            pages[content] = url
            print code, url
        elif exclude and code == 200:
            tripped = False
            exclude_extentions = [".css", ".jpg", ".js", ".png", ".gif", ".html"]
            for ext in exclude_extentions:
                if url.endswith(ext):
                    tripped = True
            if not tripped:
                print code, url
       

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("mitmproxy_urls", help="path to mitmproxy_urls")
    parser.add_argument("-x", "--exclude", action="store_true", help="exclude shit")
    args = parser.parse_args()
    main(args.mitmproxy_urls, args.exclude)
